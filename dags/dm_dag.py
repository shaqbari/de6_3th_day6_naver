from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.exceptions import AirflowSkipException
from airflow.hooks.postgres_hook import PostgresHook
from psycopg2.extras import execute_values
from datetime import datetime, timedelta
import pandas as pd
import logging
import pytz
import re
import os

default_args = {
    'start_date': datetime(2025, 6, 9),
    'retries': 1,
    'retry_delay': timedelta(minutes=10)
}

def extract_from_dw(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    conn = hook.get_conn()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS public.dm_tmp_filtered_price AS
        SELECT * FROM public.ndw WHERE false;
    """)
    conn.commit()

    cur.execute("TRUNCATE TABLE public.dm_tmp_filtered_price;")

    cur.execute("""
        INSERT INTO public.dm_tmp_filtered_price
        SELECT *
        FROM public.ndw
        WHERE productid IN (
            SELECT productid
            FROM public.ndw
            GROUP BY productid
            HAVING COUNT(DISTINCT lprice) > 1
        );
    """)
    conn.commit()

    cur.close()
    conn.close()
    logging.info("ðŸ“¦ DW â†’ dm_tmp_filtered_price í…Œì´ë¸” ìµœì‹ í™” ì™„ë£Œ")


def preprocess_dm_data(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    engine = hook.get_sqlalchemy_engine()

    df = pd.read_sql("SELECT * FROM public.dm_tmp_filtered_price", engine)

    if df.empty:
        raise AirflowSkipException("â›” ì „ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    df['dt'] = pd.to_datetime(df['dt'], utc=True).dt.tz_convert('Asia/Seoul').dt.tz_localize(None)
    df.sort_values(by=['productid', 'dt'], inplace=True)

    summary_list = []

    for productid, group in df.groupby('productid'):
        group = group.sort_values('dt')
        first_row = group.iloc[0]
        last_row = group.iloc[-1]
        min_price = group['lprice'].min()
        max_price = group['lprice'].max()
        avg_price = int(group['lprice'].mean())

        min_price_dt = group[group['lprice'] == min_price]['dt'].max()
        max_price_dt = group[group['lprice'] == max_price]['dt'].max()

        summary = {
            'productid': productid,
            'title': last_row['title'],
            'dt': last_row['dt'],
            'link': last_row['link'],
            'keyword': last_row['keyword'],
            'keyword_type': last_row['keyword_type'],
            'first_price': int(first_row['lprice']),
            'first_price_dt': first_row['dt'],
            'last_price': int(last_row['lprice']),
            'min_price': int(min_price),
            'min_price_dt': min_price_dt,
            'max_price': int(max_price),
            'max_price_dt': max_price_dt,
            'avg_price': avg_price,
            'price_diff': int(last_row['lprice'] - first_row['lprice']),
            'price_diff_pct': round((last_row['lprice'] - first_row['lprice']) / first_row['lprice'] * 100, 2) if first_row['lprice'] else 0.0
        }

        summary_list.append(summary)

    df_summary = pd.DataFrame(summary_list)
    df_summary.to_sql('dm_tmp_price_summary', engine, schema='public', if_exists='replace', index=False)
    logging.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ â†’ dm_tmp_price_summary í…Œì´ë¸” ì €ìž¥ ({len(df_summary)}ê±´)")



def insert_dm_data(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    conn = hook.get_conn()
    cur = conn.cursor()

    df = pd.read_sql("SELECT * FROM public.dm_tmp_price_summary", conn)

    if df.empty:
        logging.warning("â›” DM í…Œì´ë¸”ì— ì ìž¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    cur.execute("""
        CREATE TABLE IF NOT EXISTS public.ndm (
            productid TEXT PRIMARY KEY,
            title TEXT,
            dt TIMESTAMP,
            link TEXT,
            keyword TEXT,
            keyword_type TEXT,
            first_price BIGINT,
            first_price_dt TIMESTAMP,
            last_price BIGINT,
            min_price BIGINT,
            min_price_dt TIMESTAMP,
            max_price BIGINT,
            max_price_dt TIMESTAMP,
            avg_price BIGINT,
            price_diff BIGINT,
            price_diff_pct FLOAT
        );
    """)
    conn.commit()

    rows = df[[
        'productid', 'title', 'dt', 'link', 'keyword', 'keyword_type',
        'first_price', 'first_price_dt', 'last_price',
        'min_price', 'min_price_dt', 'max_price', 'max_price_dt',
        'avg_price', 'price_diff', 'price_diff_pct'
    ]].values.tolist()

    execute_values(cur, """
        INSERT INTO public.ndm (
            productid, title, dt, link, keyword, keyword_type,
            first_price, first_price_dt, last_price,
            min_price, min_price_dt, max_price, max_price_dt,
            avg_price, price_diff, price_diff_pct
        ) VALUES %s
        ON CONFLICT (productid) DO UPDATE SET
            title = EXCLUDED.title,
            dt = EXCLUDED.dt,
            link = EXCLUDED.link,
            keyword = EXCLUDED.keyword,
            keyword_type = EXCLUDED.keyword_type,
            first_price = EXCLUDED.first_price,
            first_price_dt = EXCLUDED.first_price_dt,
            last_price = EXCLUDED.last_price,
            min_price = EXCLUDED.min_price,
            min_price_dt = EXCLUDED.min_price_dt,
            max_price = EXCLUDED.max_price,
            max_price_dt = EXCLUDED.max_price_dt,
            avg_price = EXCLUDED.avg_price,
            price_diff = EXCLUDED.price_diff,
            price_diff_pct = EXCLUDED.price_diff_pct;
    """, rows)

    conn.commit()
    cur.close()
    conn.close()
    logging.info(f"âœ… DM í…Œì´ë¸”(dm_naver_price)ì— {len(df)}ê±´ ì ìž¬ ì™„ë£Œ")


with DAG(
    dag_id='dm_dag',
    default_args=default_args,
    schedule_interval='@hourly',
    catchup=False,
    tags=['dm', 'dw_to_dm', 'table_load']
) as dag:

    t1 = ExternalTaskSensor(
        task_id='wait_for_dw_dag',
        external_dag_id='dw_dag',
        external_task_id='transfer_to_dw',
        allowed_states=['success'],
        failed_states=['failed', 'skipped'],
        execution_delta=timedelta(hours=0),
        mode='reschedule',
        poke_interval=60,
        timeout=600
    )

    t2 = PythonOperator(
        task_id='extract_from_dw',
        python_callable=extract_from_dw,
    )

    t3 = PythonOperator(
        task_id='preprocess_dm_data',
        python_callable=preprocess_dm_data,
    )

    t4 = PythonOperator(
        task_id='insert_dm_data',
        python_callable=insert_dm_data,
    )

    t1 >> t2 >> t3 >> t4
