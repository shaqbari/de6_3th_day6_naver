from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.exceptions import AirflowSkipException
from airflow.hooks.postgres_hook import PostgresHook
from psycopg2.extras import execute_values
from datetime import datetime, timedelta
import pandas as pd
import logging
import requests
import pytz
import re
import os

default_args = {
    'start_date': datetime(2025, 6, 9),
    'retries': 1,
    'retry_delay': timedelta(minutes=10)
}

def extract_from_dw(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    conn = hook.get_conn()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS public.dm_tmp_filtered_price AS
        SELECT * FROM public.ndw WHERE false;
    """)
    conn.commit()

    cur.execute("TRUNCATE TABLE public.dm_tmp_filtered_price;")

    cur.execute("""
        INSERT INTO public.dm_tmp_filtered_price
        SELECT *
        FROM public.ndw
        WHERE productid IN (
            SELECT productid
            FROM public.ndw
            GROUP BY productid
            HAVING COUNT(DISTINCT lprice) > 1
        );
    """)
    conn.commit()

    cur.close()
    conn.close()
    logging.info("ğŸ“¦ DW â†’ dm_tmp_filtered_price í…Œì´ë¸” ìµœì‹ í™” ì™„ë£Œ")


def preprocess_dm_data(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    engine = hook.get_sqlalchemy_engine()

    df = pd.read_sql("SELECT * FROM public.dm_tmp_filtered_price", engine)

    if df.empty:
        raise AirflowSkipException("â›” ì „ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    df['dt'] = pd.to_datetime(df['dt'], utc=True).dt.tz_convert('Asia/Seoul').dt.tz_localize(None)
    df.sort_values(by=['productid', 'dt'], inplace=True)

    summary_list = []

    for productid, group in df.groupby('productid'):
        group = group.sort_values('dt')
        first_row = group.iloc[0]
        last_row = group.iloc[-1]
        min_price = group['lprice'].min()
        max_price = group['lprice'].max()
        avg_price = int(group['lprice'].mean())

        min_price_dt = group[group['lprice'] == min_price]['dt'].max()
        max_price_dt = group[group['lprice'] == max_price]['dt'].max()

        summary = {
            'productid': productid,
            'title': last_row['title'],
            'dt': last_row['dt'],
            'link': last_row['link'],
            'keyword': last_row['keyword'],
            'keyword_type': last_row['keyword_type'],
            'first_price': int(first_row['lprice']),
            'first_price_dt': first_row['dt'],
            'last_price': int(last_row['lprice']),
            'min_price': int(min_price),
            'min_price_dt': min_price_dt,
            'max_price': int(max_price),
            'max_price_dt': max_price_dt,
            'avg_price': avg_price,
            'price_diff': int(last_row['lprice'] - first_row['lprice']),
            'price_diff_pct': round((last_row['lprice'] - first_row['lprice']) / first_row['lprice'] * 100, 2) if first_row['lprice'] else 0.0
        }

        summary_list.append(summary)

    df_summary = pd.DataFrame(summary_list)
    df_summary.to_sql('dm_tmp_price_summary', engine, schema='public', if_exists='replace', index=False)
    logging.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ â†’ dm_tmp_price_summary í…Œì´ë¸” ì €ì¥ ({len(df_summary)}ê±´)")

"""
def cluster_and_dedup(**context):
    import pandas as pd
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity
    from airflow.hooks.postgres_hook import PostgresHook
    import logging

    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    engine = hook.get_sqlalchemy_engine()

    df = pd.read_sql("SELECT * FROM public.dm_tmp_price_summary", engine)

    if df.empty:
        logging.warning("â›” í´ëŸ¬ìŠ¤í„°ë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    tfidf = TfidfVectorizer()
    tfidf_matrix = tfidf.fit_transform(df['title'])
    cosine_sim = cosine_similarity(tfidf_matrix)
    cosine_dist = 1 - cosine_sim

    dbscan = DBSCAN(metric='precomputed', eps=0.2, min_samples=1)
    df['cluster_id'] = dbscan.fit_predict(cosine_dist)

    # ìµœì†Œê°€ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    min_price_per_group = df.groupby('cluster_id')['last_price'].min().reset_index()
    min_price_per_group.rename(columns={'last_price': 'min_price'}, inplace=True)
    df = df.merge(min_price_per_group, on='cluster_id')
    df = df[df['last_price'] == df['min_price']].copy()
    df = df.drop_duplicates(subset=['cluster_id', 'last_price'], keep='first')
    df.drop(columns=['min_price'], inplace=True)

    # ì €ì¥
    df.to_sql('dm_tmp_price_summary_deduped', engine, schema='public', if_exists='replace', index=False)
    logging.info(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ë° ìµœì†Œê°€ ê¸°ì¤€ dedup ì™„ë£Œ. ìµœì¢… ìƒí’ˆ ìˆ˜: {len(df)}")
    context['ti'].xcom_push(key='deduplicated_data', value=df.to_dict(orient='records'))
"""


def insert_dm_data(**context):
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    conn = hook.get_conn()
    cur = conn.cursor()

    df = pd.read_sql("SELECT * FROM public.dm_tmp_price_summary", conn)

    if df.empty:
        logging.warning("â›” DM í…Œì´ë¸”ì— ì ì¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    cur.execute("""
        CREATE TABLE IF NOT EXISTS public.ndm (
            productid TEXT PRIMARY KEY,
            title TEXT,
            dt TIMESTAMP,
            link TEXT,
            keyword TEXT,
            keyword_type TEXT,
            first_price BIGINT,
            first_price_dt TIMESTAMP,
            last_price BIGINT,
            min_price BIGINT,
            min_price_dt TIMESTAMP,
            max_price BIGINT,
            max_price_dt TIMESTAMP,
            avg_price BIGINT,
            price_diff BIGINT,
            price_diff_pct FLOAT
        );
    """)
    conn.commit()

    rows = df[[
        'productid', 'title', 'dt', 'link', 'keyword', 'keyword_type',
        'first_price', 'first_price_dt', 'last_price',
        'min_price', 'min_price_dt', 'max_price', 'max_price_dt',
        'avg_price', 'price_diff', 'price_diff_pct'
    ]].values.tolist()

    execute_values(cur, """
        INSERT INTO public.ndm (
            productid, title, dt, link, keyword, keyword_type,
            first_price, first_price_dt, last_price,
            min_price, min_price_dt, max_price, max_price_dt,
            avg_price, price_diff, price_diff_pct
        ) VALUES %s
        ON CONFLICT (productid) DO UPDATE SET
            title = EXCLUDED.title,
            dt = EXCLUDED.dt,
            link = EXCLUDED.link,
            keyword = EXCLUDED.keyword,
            keyword_type = EXCLUDED.keyword_type,
            first_price = EXCLUDED.first_price,
            first_price_dt = EXCLUDED.first_price_dt,
            last_price = EXCLUDED.last_price,
            min_price = EXCLUDED.min_price,
            min_price_dt = EXCLUDED.min_price_dt,
            max_price = EXCLUDED.max_price,
            max_price_dt = EXCLUDED.max_price_dt,
            avg_price = EXCLUDED.avg_price,
            price_diff = EXCLUDED.price_diff,
            price_diff_pct = EXCLUDED.price_diff_pct;
    """, rows)

    conn.commit()
    cur.close()
    conn.close()
    logging.info(f"âœ… DM í…Œì´ë¸”(dm_naver_price)ì— {len(df)}ê±´ ì ì¬ ì™„ë£Œ")


# Slack ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
def send_slack_message(message: str):
    SLACK_URL = Variable.get("SLACK_WEBHOOK_URL")
    requests.post(SLACK_URL, json={"text": message})

# ì•Œë¦¼ ì „ì†¡ task
def alert_slack_task(**kwargs):
    import pandas as pd
    
    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    engine = hook.get_sqlalchemy_engine()

    df = pd.read_sql("SELECT * FROM ndm", con=engine)

    if df.empty:
        print("âš ï¸ ì•Œë¦¼ ëŒ€ìƒ ìƒí’ˆ ì—†ìŒ.")
        return

    # ì¡°ê±´ í•„í„°ë§: í˜„ì¬ê°€ê°€ í‰ê· ê°€ ë˜ëŠ” ìµœê³ ê°€ ëŒ€ë¹„ 20% ì´ìƒ í•˜ë½í•œ ê²½ìš°
    filtered_df = df[
        (df['last_price'] <= df['max_price'] * 0.8) |
        (df['last_price'] <= df['avg_price'] * 0.8)
    ]

    if filtered_df.empty:
        print("âš ï¸ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for _, row in filtered_df.iterrows():
        max_drop = -(row['max_price'] - row['last_price'])
        max_drop_pct = -((max_drop / row['max_price']) * 100)

        msg = f"""ğŸ“¢ *[{row['title']}]*  
ğŸ”— <{row['link']}|ìƒí’ˆ ë³´ëŸ¬ê°€ê¸°>  
ğŸ›’ í‚¤ì›Œë“œ: {row['keyword']} / {row['keyword_type']}  
ğŸ•˜ ë¶„ì„ ê¸°ì¤€ ì‹œì : {pd.to_datetime(row['dt']).strftime('%Y-%m-%d')}  

ğŸ’° ìµœì´ˆê°€: {row['first_price']:,}ì› ({pd.to_datetime(row['first_price_dt']).strftime('%Y-%m-%d')})  
ğŸ“‰ ìµœì €ê°€: {row['min_price']:,}ì› ({pd.to_datetime(row['min_price_dt']).strftime('%Y-%m-%d')})  
ğŸ“ˆ ìµœê³ ê°€: {row['max_price']:,}ì› ({pd.to_datetime(row['max_price_dt']).strftime('%Y-%m-%d')})  
ğŸ§® í‰ê· ê°€: {row['avg_price']:,}ì›  
ğŸ’¸ í˜„ì¬ê°€: {row['last_price']:,}ì›  

ğŸ”» ìµœì´ˆê°€ ëŒ€ë¹„: {row['price_diff']:,}ì› ({row['price_diff_pct']:.2f}%)  
ğŸ”» ìµœê³ ê°€ ëŒ€ë¹„: {max_drop:,}ì› ({max_drop_pct:.2f}%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
        send_slack_message(msg)


with DAG(
    dag_id='dm_dag',
    default_args=default_args,
    schedule_interval='@hourly',
    catchup=False,
    tags=['dm', 'dw_to_dm', 'table_load']
) as dag:

    t1 = ExternalTaskSensor(
        task_id='wait_for_dw_dag',
        external_dag_id='dw_dag',
        external_task_id='transfer_to_dw',
        allowed_states=['success'],
        failed_states=['failed', 'skipped'],
        execution_delta=timedelta(hours=0),
        mode='reschedule',
        poke_interval=60,
        timeout=600
    )

    t2 = PythonOperator(
        task_id='extract_from_dw',
        python_callable=extract_from_dw,
    )

    t3 = PythonOperator(
        task_id='preprocess_dm_data',
        python_callable=preprocess_dm_data,
    )

    t4 = PythonOperator(
        task_id='insert_dm_data',
        python_callable=insert_dm_data,
    )

    t5 = PythonOperator(
        task_id='send_slack_alert',
        python_callable=alert_slack_task,
    )

    t1 >> t2 >> t3 >> t4 >> t5

"""
ì‚½ì…í•˜ë ¤ë©´ t4 ìœ„ì¹˜ì— ì‚½ì…í•˜ê³ , ê¸°ì¡´ t4, t5ë¥¼ í•œë‹¨ê³„ì”© ë¯¸ë¤„ì•¼ í•¨
    t4 = PythonOperator(
        task_id='cluster_and_dedup',
        python_callable=cluster_and_dedup,
        provide_context=True
    )
"""
