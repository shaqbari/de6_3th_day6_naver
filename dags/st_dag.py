from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook
from datetime import datetime, timedelta
from pytz import timezone
import pandas as pd
import requests
import json
import logging
import time

KST = timezone('Asia/Seoul')

default_args = {
    'start_date': datetime(2025, 6, 6),
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
}

def load_keywords(**context):
    try:
        keyword_map = json.loads(Variable.get("NAVER_KEYWORD_MAP"))
        context['ti'].xcom_push(key='keyword_map', value=keyword_map)
        logging.info(f"âœ… í‚¤ì›Œë“œ ë¡œë”© ì„±ê³µ: ì´ {len(keyword_map)}ê°œ í‚¤ì›Œë“œ")
    except Exception as e:
        logging.error(f"âŒ í‚¤ì›Œë“œ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise


def collect_data(**context):
    keyword_map = context['ti'].xcom_pull(key='keyword_map', task_ids='load_keywords')
    keywords = list(keyword_map.keys())
    all_items = []

    logging.info(f"ðŸ” API í˜¸ì¶œ ëŒ€ìƒ í‚¤ì›Œë“œ ìˆ˜: {len(keywords)}")

    for keyword in keywords:
        result = None
        for attempt in range(3):
            result = naver_search_shopping(keyword)
            if result:
                break
            logging.warning(f"âš  API ì‹¤íŒ¨, ìž¬ì‹œë„ {attempt + 1}/3: {keyword}")
            time.sleep(2)
        if not result:
            logging.error(f"âŒ ìµœì¢… API ì‹¤íŒ¨: {keyword}")
            continue

        now = datetime.now(KST)
        for item in result.get('items', []):
            item['keyword'] = keyword
            item['dt'] = now
            item['keyword_type'] = keyword_map[keyword]
            all_items.append(item)

    df = pd.DataFrame(all_items)

    if df.empty:
        logging.warning("âš  ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
    else:
        logging.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê±´")
        logging.info(f"ðŸ“ ì˜ˆì‹œ:\n{df.head(3).to_string(index=False)}")

    context['ti'].xcom_push(key='collected_df', value=df.to_json(orient='records', date_format='iso'))


def insert_to_postgres(**context):
    df_json = context['ti'].xcom_pull(key='collected_df', task_ids='collect_data')
    df = pd.read_json(df_json)

    if df.empty:
        logging.warning("â›” ì ìž¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ðŸ“¥ ì ìž¬ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜: {len(df)}")

    hook = PostgresHook(postgres_conn_id='my_postgres_conn_id')
    conn = hook.get_conn()
    cur = conn.cursor()

    # âœ… í…Œì´ë¸” ì¡´ìž¬ ì—¬ë¶€ í™•ì¸ í›„ CREATE
    cur.execute("""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' AND table_name = 'nst'
        );
    """)
    exists = cur.fetchone()[0]

    if not exists:
        logging.info("ðŸ“‚ í…Œì´ë¸”ì´ ì¡´ìž¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        cur.execute("""
            CREATE TABLE public.nst (
                dt TIMESTAMP,
                keyword_type TEXT,
                keyword TEXT,
                brand TEXT,
                category1 TEXT,
                category2 TEXT,
                category3 TEXT,
                category4 TEXT,
                hprice BIGINT,
                image TEXT,
                link TEXT,
                lprice BIGINT,
                maker TEXT,
                mallName TEXT,
                productId TEXT,
                productType INT,
                title TEXT
            );
        """)
    else:
        logging.info("ðŸ“‚ í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ìž¬í•©ë‹ˆë‹¤.")

    cur.execute("TRUNCATE TABLE public.nst;")
    logging.info("ðŸ§¹ ê¸°ì¡´ ë°ì´í„° TRUNCATE ì™„ë£Œ")

    success_count = 0
    for _, row in df.iterrows():
        try:
            cur.execute("""
                INSERT INTO public.nst (
                    dt, keyword_type, keyword,
                    brand, category1, category2, category3, category4,
                    hprice, image, link, lprice,
                    maker, mallName, productId, productType, title
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
            """, (
                row.get('dt'),
                row.get('keyword_type'),
                row.get('keyword'),
                row.get('brand'),
                row.get('category1'),
                row.get('category2'),
                row.get('category3'),
                row.get('category4'),
                int(row.get('hprice', 0)) if row.get('hprice') else 0,
                row.get('image'),
                row.get('link'),
                int(row.get('lprice', 0)) if row.get('lprice') else 0,
                row.get('maker'),
                row.get('mallName'),
                row.get('productId'),
                row.get('productType'),
                row.get('title'),
            ))
            success_count += 1
        except Exception as e:
            logging.error(f"â— INSERT ì‹¤íŒ¨: {e}")

    conn.commit()
    cur.close()
    conn.close()

    logging.info(f"âœ… ì´ {success_count}ê±´ ë°ì´í„° ì ìž¬ ì™„ë£Œ")



def naver_search_shopping(query, display=100):
    url = "https://openapi.naver.com/v1/search/shop.json"
    headers = {
        "X-Naver-Client-Id": Variable.get("NAVER_CLIENT_ID"),
        "X-Naver-Client-Secret": Variable.get("NAVER_CLIENT_SECRET"),
    }
    params = {
        'query': query,
        'display': display,
        'sort': 'sim',
        'exclude': 'used:cbshop:rental'
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code != 200:
        logging.warning(f"âš  API ì‘ë‹µ ì‹¤íŒ¨({response.status_code}): {query}")
        return None

    return response.json()


with DAG(
    dag_id='st_dag',
    default_args=default_args,
    schedule_interval='@hourly',
    tags=['naver', 'shopping', 'postgres'],
    catchup=False
) as dag:
    t1 = PythonOperator(
        task_id='load_keywords',
        python_callable=load_keywords
    )

    t2 = PythonOperator(
        task_id='collect_data',
        python_callable=collect_data
    )

    t3 = PythonOperator(
        task_id='insert_to_postgres',
        python_callable=insert_to_postgres
    )

    t1 >> t2 >> t3
